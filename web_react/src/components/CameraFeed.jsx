import React, { useEffect, useRef, useState } from "react";
import { FaceMesh } from "@mediapipe/face_mesh";
import { Hands } from "@mediapipe/hands";
import { Camera } from "@mediapipe/camera_utils";
import { drawConnectors, drawLandmarks } from "@mediapipe/drawing_utils";
import {
  FACEMESH_TESSELATION,
  FACEMESH_RIGHT_EYE,
  FACEMESH_LEFT_EYE,
  FACEMESH_FACE_OVAL,
  FACEMESH_LIPS,
} from "@mediapipe/face_mesh";
import { HAND_CONNECTIONS } from "@mediapipe/hands";
import { pipeline } from "@xenova/transformers";

const EMOTION_MAP = {
  joy: "happy",
  sadness: "sad",
  anger: "angry",
  fear: "fear",
  surprise: "surprise",
  disgust: "disgust",
  neutral: "neutral",
};

// Constants for detection
const EYE_BLINK_THRESHOLD = 0.42; // Eye Aspect Ratio threshold
const MAX_FRAMES = 120; // 4 seconds at 30fps
const HAND_FACE_DISTANCE_THRESHOLD = 0.08;

export default function CameraFeed({ sessionId, calibrated, onMetricsUpdate }) {
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const [error, setError] = useState(null);
  const [streamActive, setStreamActive] = useState(false);
  const faceMeshRef = useRef(null);
  const handsRef = useRef(null);
  const cameraRef = useRef(null);
  const resultsRef = useRef({ face: null, hands: null });
  const modelsReady = useRef({ faceMesh: false, hands: false });
  // Ref cho AI Model
  const classifierRef = useRef(null);
  const [modelLoading, setModelLoading] = useState(true);
  const lastAnalysisTime = useRef(0); // ƒê·ªÉ throttle (kh√¥ng ch·∫°y m·ªói frame)
  // TH√äM C√ÅC REF ƒê·ªÇ THEO D√ïI TR·∫†NG TH√ÅI C≈® (ƒë·ªÉ ph√°t hi·ªán thay ƒë·ªïi)
  const prevBlinkState = useRef(false);
  const prevHandState = useRef(false);

  // Ref ƒë·ªÉ ƒë·∫øm t·ªïng s·ªë l·∫ßn (Count) thay v√¨ Buffer frame
  const totalBlinks = useRef(0);
  const totalHandTouches = useRef(0);
  // 1. Ref ƒë·ªÉ ch·ª©a danh s√°ch th·ªùi ƒëi·ªÉm ch·ªõp m·∫Øt (d√πng cho Sliding Window)
  const blinkTimestamps = useRef([]);
  // 2. D√πng cho Count: ƒê·∫øm s·ªë l·∫ßn trong chu k·ª≥ 60s hi·ªán t·∫°i
  const currentCycleBlinks = useRef(0);
  const cycleStartTime = useRef(Date.now()); // M·ªëc th·ªùi gian b·∫Øt ƒë·∫ßu chu k·ª≥ 60s

  // 3. Logic ph√°t hi·ªán (Debounce/Edge detection)
  const isBlinkingRef = useRef(false);

  const lastBlinkTime = useRef(0);
  const lastHandTouchTime = useRef(0);

  // Metrics tracking
  const blinksBuffer = useRef([]);
  const handToFaceBuffer = useRef([]);
  const gazeBuffer = useRef([]);
  const frameCountRef = useRef(0);
  // H√†m t√≠nh kho·∫£ng c√°ch gi·ªØa 2 ƒëi·ªÉm (Euclidean distance)
  const getDistance = (p1, p2) => {
    return Math.sqrt(Math.pow(p1.x - p2.x, 2) + Math.pow(p1.y - p2.y, 2));
  };
  // T√≠nh to√°n Lip Compression (M√≠m m√¥i)
  // D·ª±a tr√™n logic Python: get_aspect_ratio(face[0], face[17], face[61], face[291])
  const calculateLipRatio = (landmarks) => {
    const top = landmarks[0]; // M√¥i tr√™n
    const bottom = landmarks[17]; // M√¥i d∆∞·ªõi
    const left = landmarks[61]; // Kh√≥e mi·ªáng tr√°i
    const right = landmarks[291]; // Kh√≥e mi·ªáng ph·∫£i

    const height = getDistance(top, bottom);
    const width = getDistance(left, right);

    // Tr√°nh chia cho 0
    if (width === 0) return 0;
    return height / width;
  };

  // T√≠nh to√°n Eye Gaze (H∆∞·ªõng nh√¨n)
  // D·ª±a tr√™n logic Python: so s√°nh t√¢m m·ªëng m·∫Øt v·ªõi t√¢m m·∫Øt
  const calculateGazeShift = (landmarks) => {
    // Landmarks m·∫Øt ph·∫£i (Right Eye)
    const rightIris = {
      x: (landmarks[471].x + landmarks[469].x) / 2,
      y: (landmarks[471].y + landmarks[469].y) / 2,
    };
    const rightEyeCenter = {
      x: (landmarks[33].x + landmarks[133].x) / 2,
      y: (landmarks[33].y + landmarks[133].y) / 2,
    };
    const rightEyeWidth = Math.abs(landmarks[33].x - landmarks[133].x);

    // Landmarks m·∫Øt tr√°i (Left Eye)
    const leftIris = {
      x: (landmarks[476].x + landmarks[474].x) / 2,
      y: (landmarks[476].y + landmarks[474].y) / 2,
    };
    const leftEyeCenter = {
      x: (landmarks[362].x + landmarks[263].x) / 2,
      y: (landmarks[362].y + landmarks[263].y) / 2,
    };
    const leftEyeWidth = Math.abs(landmarks[362].x - landmarks[263].x);

    // T√≠nh ƒë·ªô l·ªách (Gaze Relative)
    const rightGaze = getDistance(rightIris, rightEyeCenter) / rightEyeWidth;
    const leftGaze = getDistance(leftIris, leftEyeCenter) / leftEyeWidth;

    return (rightGaze + leftGaze) / 2;
  };

  // --- TH√äM: Load Model Hugging Face ---
  useEffect(() => {
    const loadModel = async () => {
      try {
        console.log("Loading Emotion Model...");
        // S·ª≠ d·ª•ng pipeline image-classification
        // Model g·ª£i √Ω: 'Xenova/facial_emotions_image_detection' ho·∫∑c 'Xenova/emotion-english-distilroberta-base' (cho text),
        // Cho h√¨nh ·∫£nh, ta d√πng model FER ƒë√£ convert sang ONNX.
        const classifier = await pipeline(
          "image-classification",
          "Xenova/facial_emotions_image_detection"
        );
        classifierRef.current = classifier;
        setModelLoading(false);
        console.log("‚úÖ Emotion Model Loaded!");
      } catch (err) {
        console.error("Failed to load emotion model", err);
      }
    };
    loadModel();
  }, []);

  // --- TH√äM: Logic ph√¢n t√≠ch c·∫£m x√∫c ---
  const analyzeEmotion = async (videoElement, faceLandmarks) => {
    if (!classifierRef.current || !faceLandmarks) return null;

    try {
      // 1. C·∫Øt khu√¥n m·∫∑t t·ª´ video (Bounding Box)
      // L·∫•y t·ªça ƒë·ªô min/max x,y t·ª´ landmarks
      let minX = 1,
        minY = 1,
        maxX = 0,
        maxY = 0;
      faceLandmarks.forEach((pt) => {
        if (pt.x < minX) minX = pt.x;
        if (pt.y < minY) minY = pt.y;
        if (pt.x > maxX) maxX = pt.x;
        if (pt.y > maxY) maxY = pt.y;
      });

      // Th√™m padding cho khung h√¨nh m·∫∑t
      const padding = 0.1;
      minX = Math.max(0, minX - padding);
      minY = Math.max(0, minY - padding);
      maxX = Math.min(1, maxX + padding);
      maxY = Math.min(1, maxY + padding);

      // T·∫°o canvas t·∫°m ƒë·ªÉ crop
      const tempCanvas = document.createElement("canvas");
      const tempCtx = tempCanvas.getContext("2d");
      const videoW = videoElement.videoWidth;
      const videoH = videoElement.videoHeight;

      const cropX = minX * videoW;
      const cropY = minY * videoH;
      const cropW = (maxX - minX) * videoW;
      const cropH = (maxY - minY) * videoH;

      tempCanvas.width = cropW;
      tempCanvas.height = cropH;

      // V·∫Ω ph·∫ßn m·∫∑t l√™n canvas t·∫°m
      tempCtx.drawImage(
        videoElement,
        cropX,
        cropY,
        cropW,
        cropH,
        0,
        0,
        cropW,
        cropH
      );

      // L·∫•y Data URL
      const imageMap = tempCanvas.toDataURL("image/jpeg", 0.8);

      // 2. Ch·∫°y Model Inference
      const results = await classifierRef.current(imageMap);

      // 3. Chu·∫©n h√≥a k·∫øt qu·∫£ tr·∫£ v·ªÅ format c·ªßa App
      // results d·∫°ng: [{ label: 'happy', score: 0.9 }, ...]
      const emotionData = {
        angry: 0,
        disgust: 0,
        fear: 0,
        happy: 0,
        sad: 0,
        surprise: 0,
        neutral: 0,
      };

      let dominantEmotion = "neutral";
      let maxScore = 0;

      results.forEach((res) => {
        const key = EMOTION_MAP[res.label] || res.label;
        if (emotionData.hasOwnProperty(key)) {
          emotionData[key] = res.score * 100; // ƒê·ªïi sang %
          if (res.score > maxScore) {
            maxScore = res.score;
            dominantEmotion = key;
          }
        }
      });

      return { emotionData, dominantEmotion, confidence: maxScore };
    } catch (err) {
      console.error("Emotion analysis error:", err);
      return null;
    }
  };

  useEffect(() => {
    let currentStream = null;

    const startCamera = async () => {
      try {
        // Truy c·∫≠p camera tr·ª±c ti·∫øp t·ª´ browser
        const stream = await navigator.mediaDevices.getUserMedia({
          video: {
            width: { ideal: 1280 },
            height: { ideal: 720 },
            facingMode: "user",
          },
          audio: false,
        });

        if (videoRef.current) {
          videoRef.current.srcObject = stream;
          currentStream = stream;
          setStreamActive(true);

          // Start continuous drawing loop for video
          videoRef.current.onloadedmetadata = () => {
            startDrawingLoop();
          };

          // Initialize MediaPipe FaceMesh and Hands
          initializeMediaPipe();
        }
      } catch (err) {
        console.error("Camera error:", err);
        setError("Cannot access camera. Please allow camera permission!");
      }
    };

    const initializeMediaPipe = async () => {
      try {
        console.log("üîß Initializing MediaPipe models...");

        // Initialize FaceMesh
        const faceMesh = new FaceMesh({
          locateFile: (file) => {
            return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;
          },
        });

        faceMesh.setOptions({
          maxNumFaces: 1,
          refineLandmarks: true,
          minDetectionConfidence: 0.5,
          minTrackingConfidence: 0.5,
        });

        faceMesh.onResults((results) => {
          resultsRef.current.face = results;
          // Mark model as ready on first results
          if (!modelsReady.current.faceMesh) {
            console.log("‚úÖ FaceMesh ready");
            modelsReady.current.faceMesh = true;
          }
        });
        faceMeshRef.current = faceMesh;

        // Initialize Hands
        const hands = new Hands({
          locateFile: (file) => {
            return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;
          },
        });

        hands.setOptions({
          maxNumHands: 2,
          modelComplexity: 1,
          minDetectionConfidence: 0.5,
          minTrackingConfidence: 0.5,
        });

        hands.onResults((results) => {
          resultsRef.current.hands = results;
          // Mark model as ready on first results
          if (!modelsReady.current.hands) {
            console.log("‚úÖ Hands ready");
            modelsReady.current.hands = true;
          }
        });
        handsRef.current = hands;

        // Wait for models to fully initialize before starting camera
        console.log("‚è≥ Waiting for MediaPipe WASM modules to load...");
        await new Promise((resolve) => setTimeout(resolve, 2000));

        // Start camera processing
        if (videoRef.current) {
          const camera = new Camera(videoRef.current, {
            onFrame: async () => {
              // Only send frames after models are initialized
              if (faceMeshRef.current && handsRef.current && videoRef.current) {
                try {
                  await faceMeshRef.current.send({ image: videoRef.current });
                  await handsRef.current.send({ image: videoRef.current });
                } catch (err) {
                  // Silently handle initialization errors
                  if (
                    !modelsReady.current.faceMesh ||
                    !modelsReady.current.hands
                  ) {
                    // Still initializing, suppress errors
                    return;
                  }
                  console.error("Frame processing error:", err);
                }
              }
            },
            width: 1280,
            height: 720,
          });

          console.log("üé• Starting MediaPipe camera feed...");
          camera.start();
          cameraRef.current = camera;
        }
      } catch (err) {
        console.error("MediaPipe initialization error:", err);
        setError("Failed to initialize MediaPipe. Please refresh the page.");
      }
    };

    // Continuous drawing loop for video + landmarks
    const startDrawingLoop = () => {
      const draw = () => {
        if (
          videoRef.current &&
          canvasRef.current &&
          videoRef.current.readyState === videoRef.current.HAVE_ENOUGH_DATA
        ) {
          drawResults();
        }
        requestAnimationFrame(draw);
      };
      draw();
    };

    // Helper: Calculate eye aspect ratio
    // const getEyeAspectRatio = (landmarks, eyePoints) => {
    //   const vertical1 = Math.hypot(
    //     landmarks[eyePoints[1]].x - landmarks[eyePoints[3]].x,
    //     landmarks[eyePoints[1]].y - landmarks[eyePoints[3]].y
    //   );
    //   const vertical2 = Math.hypot(
    //     landmarks[eyePoints[2]].x - landmarks[eyePoints[0]].x,
    //     landmarks[eyePoints[2]].y - landmarks[eyePoints[0]].y
    //   );
    //   const horizontal = Math.hypot(
    //     landmarks[eyePoints[0]].x - landmarks[eyePoints[3]].x,
    //     landmarks[eyePoints[0]].y - landmarks[eyePoints[3]].y
    //   );
    //   return (vertical1 + vertical2) / (2.0 * horizontal);
    // };

    // Thay th·∫ø h√†m c≈© b·∫±ng h√†m n√†y
    const getEyeAspectRatio = (landmarks, eyePoints) => {
      // eyePoints th·ª© t·ª±: [Top, Bottom, Inner, Outer]
      // Right eye: [159, 145, 133, 33]
      // Left eye: [386, 374, 362, 263]

      const top = landmarks[eyePoints[0]];
      const bottom = landmarks[eyePoints[1]];
      const inner = landmarks[eyePoints[2]];
      const outer = landmarks[eyePoints[3]];

      // T√≠nh chi·ªÅu cao m·∫Øt (Kho·∫£ng c√°ch gi·ªØa m√≠ tr√™n v√† m√≠ d∆∞·ªõi)
      const vertical = Math.hypot(top.x - bottom.x, top.y - bottom.y);

      // T√≠nh chi·ªÅu r·ªông m·∫Øt (Kho·∫£ng c√°ch gi·ªØa kh√≥e m·∫Øt trong v√† ngo√†i)
      const horizontal = Math.hypot(inner.x - outer.x, inner.y - outer.y);

      // Tr√°nh chia cho 0
      if (horizontal === 0) return 0;

      return vertical / horizontal;
    };

    // Helper: Check if blinking
    const isBlinking = (landmarks) => {
      // Right eye: 159, 145, 133, 33
      const rightEAR = getEyeAspectRatio(landmarks, [159, 145, 133, 33]);
      // Left eye: 386, 374, 362, 263
      const leftEAR = getEyeAspectRatio(landmarks, [386, 374, 362, 263]);
      const avgEAR = (rightEAR + leftEAR) / 2;
      return avgEAR < EYE_BLINK_THRESHOLD;
    };

    // Helper: Check hand-to-face contact
    const checkHandToFace = (handLandmarks, faceLandmarks) => {
      if (!handLandmarks || !faceLandmarks) return false;

      // Check key finger points (thumb tip=4, index tip=8, pinky tip=20)
      const fingerTips = [4, 8, 20];

      // Get face bounding region from FACEMESH_FACE_OVAL
      const facePoints = [
        10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365,
        379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234,
        127, 162, 21, 54, 103, 67, 109,
      ];

      for (const handLandmark of handLandmarks) {
        for (const fingerIdx of fingerTips) {
          const finger = handLandmark[fingerIdx];

          // Check distance to any face point
          for (const faceIdx of facePoints) {
            const facePoint = faceLandmarks[faceIdx];
            const distance = Math.hypot(
              finger.x - facePoint.x,
              finger.y - facePoint.y
            );

            if (distance < HAND_FACE_DISTANCE_THRESHOLD) {
              return true;
            }
          }
        }
      }
      return false;
    };

    // Calculate metrics from current landmarks
    const calculateMetrics = () => {
      frameCountRef.current++;
      const now = Date.now(); // L·∫•y th·ªùi gian hi·ªán t·∫°i

      if (now - cycleStartTime.current > 60000) {
        console.log(
          "‚è±Ô∏è 60s Cycle Reset! Prev Count:",
          currentCycleBlinks.current
        );
        currentCycleBlinks.current = 0;
        cycleStartTime.current = now;
      }

      // let blink = false;
      let handToFace = false;
      let lipCompression = false;
      let gazeShift = 0;
      let isBlinkingNow = false;
      let isTouchingFaceNow = false;
      let currentEAR = 0; // Eye Aspect Ratio

      // Blink detection
      if (
        resultsRef.current.face &&
        resultsRef.current.face.multiFaceLandmarks
      ) {
        const landmarks = resultsRef.current.face.multiFaceLandmarks[0];
        // blink = isBlinking(landmarks);
        // T√≠nh EAR chi ti·∫øt ƒë·ªÉ Debug
        // const rightEAR = getEyeAspectRatio(landmarks, [159, 145, 133, 33]);
        // const leftEAR = getEyeAspectRatio(landmarks, [386, 374, 362, 263]);
        // currentEAR = (rightEAR + leftEAR) / 2;

        // So s√°nh v·ªõi ng∆∞·ª°ng
        // if (currentEAR < EYE_BLINK_THRESHOLD) {
        //   isBlinkingNow = true;
        // }
        // Detect Blink
        isBlinkingNow = isBlinking(landmarks);
        // Logic ƒë·∫øm (Ch·ªâ tƒÉng khi chuy·ªÉn t·ª´ M·ªü -> Nh·∫Øm v√† cooldown 300ms)
        if (isBlinkingNow && !isBlinkingRef.current) {
          if (now - lastBlinkTime.current > 300) {
            // TƒÉng bi·∫øn ƒë·∫øm c·ªßa chu k·ª≥ hi·ªán t·∫°i (s·∫Ω reset v·ªÅ 0 m·ªói ph√∫t)
            currentCycleBlinks.current += 1;

            // Th√™m timestamp v√†o m·∫£ng ƒë·ªÉ t√≠nh Rate (Sliding Window)
            blinkTimestamps.current.push(now);

            lastBlinkTime.current = now;
          }
        }
        isBlinkingRef.current = isBlinkingNow;

        // Lip Compression Detection
        // Ng∆∞·ª°ng 0.35
        const lipRatio = calculateLipRatio(landmarks);
        if (lipRatio < 0.35) {
          lipCompression = true;
        }
        // Gaze Shift Detection
        gazeShift = calculateGazeShift(landmarks);
      }

      // Hand-to-face detection
      if (
        resultsRef.current.face &&
        resultsRef.current.hands &&
        resultsRef.current.face.multiFaceLandmarks &&
        resultsRef.current.hands.multiHandLandmarks
      ) {
        const faceLandmarks = resultsRef.current.face.multiFaceLandmarks[0];
        isTouchingFaceNow = checkHandToFace(
          resultsRef.current.hands.multiHandLandmarks,
          faceLandmarks
        );
      }

      // C·∫¨P NH·∫¨T T·ªîNG S·ªê L·∫¶N (COUNT) THAY V√å BUFFER FRAME
      // C·∫≠p nh·∫≠t t·ªïng s·ªë l·∫ßn nh√°y m·∫Øt
      if (isBlinkingNow && !prevBlinkState.current) {
        if (now - lastBlinkTime.current > 300) {
          // Cooldown 300ms
          totalBlinks.current += 1;
          blinkTimestamps.current.push(now);
          lastBlinkTime.current = now;
          console.log("üëÅÔ∏è Valid Blink Detected! Total:", totalBlinks.current);
        }
      }
      prevBlinkState.current = isBlinkingNow;
      // L·ªçc b·ªè c√°c l·∫ßn ch·ªõp m·∫Øt ƒë√£ qu√° 60 gi√¢y (60000ms)
      // ƒê·ªÉ t√≠nh rate ch√≠nh x√°c trong 1 ph√∫t g·∫ßn nh·∫•t
      // L·ªçc b·ªè c√°c l·∫ßn ch·ªõp qu√° 60s
      blinkTimestamps.current = blinkTimestamps.current.filter(
        (t) => now - t <= 60000
      );

      // T√≠nh Rate hi·ªán t·∫°i
      let currentBlinkRate = blinkTimestamps.current.length;
      const timeElapsedSeconds = frameCountRef.current / 30; // Gi·∫£ s·ª≠ 30fps
      if (timeElapsedSeconds < 60 && timeElapsedSeconds > 5) {
        // Ch·ªâ ∆∞·ªõc l∆∞·ª£ng n·∫øu s·ªë l·∫ßn blink > 1 ƒë·ªÉ tr√°nh nh·∫£y s·ªë qu√° l·ªõn khi m·ªõi v√†o
        if (currentBlinkRate > 1) {
          currentBlinkRate = Math.round(
            (currentBlinkRate / timeElapsedSeconds) * 60
          );
        }
      }
      // C·∫≠p nh·∫≠t t·ªïng s·ªë l·∫ßn ch·∫°m tay l√™n m·∫∑t
      if (isTouchingFaceNow && !prevHandState.current) {
        // Cooldown 2 gi√¢y ƒë·ªÉ tr√°nh ƒë·∫øm tr√πng 1 h√†nh ƒë·ªông
        if (now - lastHandTouchTime.current > 2000) {
          totalHandTouches.current += 1;
          lastHandTouchTime.current = now;
          console.log(
            "‚úã HAND TOUCH DETECTED! Total:",
            totalHandTouches.current
          );
        }
      }
      prevHandState.current = isTouchingFaceNow;

      // // Update buffers
      // blinksBuffer.current.push(blink);
      // handToFaceBuffer.current.push(handToFace);

      // // Keep buffer size limited
      // if (blinksBuffer.current.length > MAX_FRAMES) {
      //   blinksBuffer.current.shift();
      // }
      // if (handToFaceBuffer.current.length > MAX_FRAMES) {
      //   handToFaceBuffer.current.shift();
      // }

      // Trigger Emotion Analysis m·ªói 1 gi√¢y (30 frames)
      let aiEmotionResult = null;
      if (frameCountRef.current % 30 === 0 && resultsRef.current.face) {
        // M·ªói ~1s
        const landmarks = resultsRef.current.face.multiFaceLandmarks[0];
        // G·ªçi h√†m async nh∆∞ng kh√¥ng await ƒë·ªÉ tr√°nh block UI thread qu√° l√¢u
        analyzeEmotion(videoRef.current, landmarks).then((result) => {
          if (result && onMetricsUpdate) {
            // G·ª≠i update ri√™ng cho Emotion ƒë·ªÉ UI m∆∞·ª£t h∆°n
            onMetricsUpdate({
              type: "emotion_update", // ƒê√°nh d·∫•u lo·∫°i update
              emotionData: result.emotionData,
              dominantEmotion: result.dominantEmotion,
              emotionConfidence: result.confidence,
            });
          }
        });
      }

      // Calculate and emit metrics every 30 frames (1 second at 30fps)
      if (frameCountRef.current % 30 === 0 && onMetricsUpdate) {
        console.log(
          `Debug Metrics - EAR: ${currentEAR.toFixed(
            3
          )} (Threshold: ${EYE_BLINK_THRESHOLD})
          }`
        );
        // const blinkCount = blinksBuffer.current.filter((b) => b).length;
        // const handToFaceCount = handToFaceBuffer.current.filter(
        //   (h) => h
        // ).length;

        // T√≠nh Rate b·∫±ng Sliding Window:
        // L·ªçc b·ªè c√°c timestamp c≈© h∆°n 60s
        blinkTimestamps.current = blinkTimestamps.current.filter(
          (t) => now - t <= 60000
        );

        // Rate = S·ªë l∆∞·ª£ng blink c√≤n l·∫°i trong c·ª≠a s·ªï 60s
        let slidingWindowRate = blinkTimestamps.current.length;

        // Calculate per minute rates
        const secondsRecorded = blinksBuffer.current.length / 30;
        // T√≠nh ph√∫t ƒë√£ tr√¥i qua ƒë·ªÉ t√≠nh t·ªëc ƒë·ªô ch·ªõp m·∫Øt trung b√¨nh
        const minutesElapsed = frameCountRef.current / 30 / 60;

        // Blink Rate = T·ªïng s·ªë l·∫ßn ch·ªõp / s·ªë ph√∫t (tr√°nh chia cho 0)
        const calculatedBlinkRate =
          minutesElapsed > 0.1
            ? Math.round(totalBlinks.current / minutesElapsed)
            : 0;

        // const blinkRate = secondsRecorded > 0 ? (blinkCount / secondsRecorded) * 60 : 0;
        // const handToFaceFreq = secondsRecorded > 0 ? (handToFaceCount / secondsRecorded) * 60 : 0;

        onMetricsUpdate({
          type: "basic_update",
          blinkRate: slidingWindowRate, // T·ªëc ƒë·ªô trung b√¨nh (l·∫ßn/ph√∫t)
          // blinkCount: totalBlinks.current, // T·ªïng s·ªë l·∫ßn ch·ªõp t·ª´ ƒë·∫ßu bu·ªïi
          blinkCount: currentCycleBlinks.current, // T·ªïng s·ªë l·∫ßn ch·ªõp trong chu k·ª≥ 60s hi·ªán t·∫°i
          // handToFaceFrequency: Math.round(handToFaceFreq * 10) / 10,
          // currentBlink: blink,
          // currentBlink: isBlinkingNow,
          // currentHandToFace: handToFace,
          handTouchTotal: totalHandTouches.current,
          currentHandToFace: isTouchingFaceNow,
          // handToFaceCount: totalHandTouches.current, // T·ªïng s·ªë l·∫ßn ch·∫°m tay l√™n m·∫∑t
          isLipCompressed: lipCompression, // True/False
          gazeShiftIntensity: gazeShift, // Float (ƒë·ªô l·ªõn c·ªßa vi·ªác ƒë·∫£o m·∫Øt)
          frameCount: frameCountRef.current,
        });
      }
    };

    const drawResults = () => {
      if (!canvasRef.current || !videoRef.current) return;

      const canvas = canvasRef.current;
      const ctx = canvas.getContext("2d");

      // Set canvas size to match video
      if (
        canvas.width !== videoRef.current.videoWidth ||
        canvas.height !== videoRef.current.videoHeight
      ) {
        canvas.width = videoRef.current.videoWidth;
        canvas.height = videoRef.current.videoHeight;
      }

      // Save context state
      ctx.save();

      // Clear canvas
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      // IMPORTANT: Always draw video first (background)
      if (videoRef.current.videoWidth > 0) {
        ctx.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height);
      }

      // Draw face landmarks
      if (
        resultsRef.current.face &&
        resultsRef.current.face.multiFaceLandmarks
      ) {
        for (const landmarks of resultsRef.current.face.multiFaceLandmarks) {
          // Draw face mesh tesselation (l∆∞·ªõi khu√¥n m·∫∑t)
          drawConnectors(ctx, landmarks, FACEMESH_TESSELATION, {
            color: "#C0C0C070",
            lineWidth: 0.5,
          });

          // Draw face oval (ƒë∆∞·ªùng vi·ªÅn m·∫∑t)
          drawConnectors(ctx, landmarks, FACEMESH_FACE_OVAL, {
            color: "#E0E0E0",
            lineWidth: 1,
          });

          // Draw eyes (m·∫Øt)
          drawConnectors(ctx, landmarks, FACEMESH_RIGHT_EYE, {
            color: "#FF3030",
            lineWidth: 1,
          });
          drawConnectors(ctx, landmarks, FACEMESH_LEFT_EYE, {
            color: "#30FF30",
            lineWidth: 1,
          });

          // Draw lips (m√¥i)
          drawConnectors(ctx, landmarks, FACEMESH_LIPS, {
            color: "#E0E0E0",
            lineWidth: 1,
          });
        }
      }

      // Draw hand landmarks
      if (
        resultsRef.current.hands &&
        resultsRef.current.hands.multiHandLandmarks
      ) {
        for (const landmarks of resultsRef.current.hands.multiHandLandmarks) {
          // Draw hand connections (ƒë∆∞·ªùng n·ªëi ng√≥n tay)
          drawConnectors(ctx, landmarks, HAND_CONNECTIONS, {
            color: "#00FF00",
            lineWidth: 2,
          });

          // Draw hand landmarks (c√°c ƒëi·ªÉm tr√™n b√†n tay)
          drawLandmarks(ctx, landmarks, {
            color: "#FF0000",
            lineWidth: 1,
            radius: 3,
          });
        }
      }

      // Restore context state
      ctx.restore();

      // Calculate metrics from landmarks (with error handling)
      try {
        if (modelsReady.current.faceMesh && modelsReady.current.hands) {
          calculateMetrics();
        }
      } catch (err) {
        console.error("Error calculating metrics:", err);
      }
    };

    startCamera();

    return () => {
      if (currentStream) {
        currentStream.getTracks().forEach((track) => track.stop());
      }
      if (cameraRef.current) {
        cameraRef.current.stop();
      }
    };
  }, []);

  return (
    <div className="camera-feed-container bg-gray-800 rounded-lg overflow-hidden">
      <div className="aspect-video bg-gray-700 relative">
        {error && (
          <div className="absolute top-2 left-2 right-2 text-red-400 text-xs bg-red-900/80 p-2 rounded z-10">
            {error}
          </div>
        )}

        {modelLoading && (
          <div className="absolute top-2 right-2 bg-blue-600 text-white text-xs px-2 py-1 rounded z-20 animate-pulse">
            Loading AI Model...
          </div>
        )}

        {!streamActive && !error && (
          <div className="absolute inset-0 flex items-center justify-center">
            <div className="text-gray-500 text-sm animate-pulse">
              Accessing camera...
            </div>
          </div>
        )}

        {/* Video element - direct camera feed (hidden, used for processing) */}
        <video ref={videoRef} autoPlay playsInline muted className="hidden" />

        {/* Canvas overlay - displays video + landmarks */}
        <canvas
          ref={canvasRef}
          className="w-full h-full object-cover transform scale-x-[-1]"
        />

        {/* Status overlay */}
        <div className="absolute bottom-2 left-2 text-xs text-gray-300 bg-black/50 px-2 py-1 rounded z-10">
          <span className={calibrated ? "text-green-400" : "text-yellow-400"}>
            {calibrated ? "‚óè ANALYZING" : "‚óè CALIBRATING"}
          </span>
        </div>
      </div>
    </div>
  );
}
